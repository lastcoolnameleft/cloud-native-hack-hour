

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Resilience &mdash; Portworx Microsoft GBB Labs 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/bespoke.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinxmark.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Functional Testing" href="ch_functional_testing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Portworx Microsoft GBB Labs
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ch_prerequisites.html">Portworx Microsoft Azure Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch_functional_testing.html">Functional Testing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Resilience</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#node-failure">Node failure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#node-shutdown-no-quorum-lost">Node shutdown - no quorum lost</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Portworx Microsoft GBB Labs</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Resilience</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="resilience">
<h1>Resilience<a class="headerlink" href="#resilience" title="Permalink to this headline">¶</a></h1>
<p>In this section we will show how Portworx provides high availability in the case of node, network, and drive failures.</p>
<div class="section" id="node-failure">
<h2>Node failure<a class="headerlink" href="#node-failure" title="Permalink to this headline">¶</a></h2>
<p>When a Kubernetes node fails it can take up to 5 minutes for a pod to be rescheduled. This is due to a combination of the following default Kubernetes settings:</p>
<blockquote>
<div><ul class="simple">
<li><p>kubelet <cite>node-status-update-frequency</cite>: Specifies how often kubelet posts node status to master (default 10s)</p></li>
<li><p>kube-controller-manager <cite>node-monitor-period</cite>: The period for syncing NodeStatus in NodeController (default 5s)</p></li>
<li><p>kube-controller-manager <cite>node-monitor-grace-period</cite>: Amount of time which we allow running Node to be unresponsive before marking it unhealthy (default 40s)</p></li>
<li><p>kube-controller-manager <cite>pod-eviction-timeout</cite>: The grace period for deleting pods on failed nodes (default 5m0s)</p></li>
</ul>
</div></blockquote>
<p>5 minutes is too long to wait for a database to come back online, that is why Portworx provides a mechanism that allows for Pods to come back online much quicker. This is done by Stork when it is configured to schedule your pods. Remember the <cite>spec.stork.args.webhook-controller</cite> setting on your <cite>StorageCluster</cite> spec? That’s to ensure Stork is the scheduler on any Pod that has mounted a Portworx volume. Let’s check that it’s set to true:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get stc -n portworx cluster-1 -ojsonpath<span class="o">=</span><span class="s1">&#39;{.spec.stork.args.webhook-controller}&#39;</span>
</pre></div>
</div>
<p>If you the above command doesn’t return true that means you need to add the webhook controller arg:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">args</span><span class="o">=</span><span class="sb">`</span>kubectl get stc cluster-1 -n portworx -ojsonpath<span class="o">=</span><span class="s1">&#39;{.spec.stork.args}&#39;</span><span class="sb">`</span>
<span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;</span><span class="nv">$args</span><span class="s2">&quot;</span> <span class="o">]</span>
<span class="k">then</span>
  kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args&quot;, &quot;value&quot;: {&quot;webhook-controller&quot;: &quot;true&quot;}}]&#39;</span>
<span class="k">else</span>
  kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args/webhook-controller&quot;, &quot;value&quot;: &quot;true&quot;}]&#39;</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>Now that stork is configured as the scheduler for all pods using Portworx volumes, we can configure it with an aggressive <cite>health-monitor-interval</cite> setting of 30 seconds:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args/health-monitor-interval&quot;, &quot;value&quot;: &quot;30&quot;}]&#39;</span>
</pre></div>
</div>
<p>OK, so now we are all set to perform a node failover test.</p>
<p>First, let’s find out which node our postgres database is running on:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>kubectl get pods -l <span class="nv">app</span><span class="o">=</span>postgres -n postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].spec.nodeName}&#39;</span><span class="sb">`</span>
<span class="nb">echo</span> <span class="nv">$NODE</span>
</pre></div>
</div>
<p>To compare how fast we can failover a Pod with a portworx volume to a Pod that doesn’t have a Portworx volume we will deploy an nginx pod on the same host. What we want is to make sure nginx will initially run on the same node as postgres but then be free to move to any other node when the node fails. We will use a nodeSelector for this purpose. First, let’s label our node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl label node <span class="nv">$NODE</span> px-poc/run<span class="o">=</span><span class="s2">&quot;here&quot;</span>
</pre></div>
</div>
<p>Now let’s create an nginx deployment with the nodeSelector for that label:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">namespace</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">postgres</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">selector</span><span class="p">:</span>
  <span class="nt">matchLabels</span><span class="p">:</span>
    <span class="nt">app</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">template</span><span class="p">:</span>
  <span class="nt">metadata</span><span class="p">:</span>
    <span class="nt">labels</span><span class="p">:</span>
    <span class="nt">app</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">spec</span><span class="p">:</span>
    <span class="nt">nodeSelector</span><span class="p">:</span>
    <span class="nt">px-poc/run</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">here</span>
    <span class="nt">containers</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
    <span class="nt">imagePullPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
    <span class="nt">ports</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">containerPort</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
      <span class="nt">protocol</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TCP</span>
    <span class="nt">restartPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Now confirm that both pods are running on the same node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po -n postgres -owide
</pre></div>
</div>
<p>Now let’s add the <cite>px-poc/run=”here”</cite> label to at least one more worker node so that nginx can failover:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">OTHER_NODE</span><span class="o">=</span><span class="sb">`</span>kubectl get nodes -l node-role.kubernetes.io/worker<span class="o">=</span><span class="nb">true</span> --no-headers <span class="p">|</span> grep -v <span class="nv">$NODE</span> <span class="p">|</span> head -n <span class="m">1</span> <span class="p">|</span> awk <span class="s1">&#39;{print $1}&#39;</span><span class="sb">`</span>
kubectl label node <span class="nv">$OTHER_NODE</span> px-poc/run<span class="o">=</span><span class="s2">&quot;here&quot;</span>
</pre></div>
</div>
<p>Now we are ready to perform the resilience test, shutdown the node where nginx and postgres are running and see how fast they failover by running a watch command on:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch kubectl get po -owide -n postgres
</pre></div>
</div>
<p>In our test results below you can see by the age of the postgres pod that it failed over a 4m8s faster than nginx, the failover times in our test were 1m30s and 5m38s respectively:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>NAME                       READY   STATUS        RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
nginx-6598b75bbf-zqrxv     <span class="m">1</span>/1     Running       <span class="m">0</span>          18s     <span class="m">10</span>.44.0.5   aws-node-1   &lt;none&gt;           &lt;none&gt;
postgres-ddf7d7dfc-thnx4   <span class="m">1</span>/1     Running       <span class="m">0</span>          4m26s   <span class="m">10</span>.44.0.3   aws-node-1   &lt;none&gt;           &lt;none&gt;
</pre></div>
</div>
<p>Finally, let’s delete the nginx pod and remove our labels:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl delete deploy nginx -n postgres
kubectl label node <span class="nv">$NODE</span> <span class="nv">$OTHER_NODE</span> px-poc/run-
</pre></div>
</div>
</div>
<div class="section" id="node-shutdown-no-quorum-lost">
<h2>Node shutdown - no quorum lost<a class="headerlink" href="#node-shutdown-no-quorum-lost" title="Permalink to this headline">¶</a></h2>
<p>In the previous test we shutdown one of our hosts, because we installed on a minimum of 3 nodes we can verify that the cluster has not lost quorum:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl pxc node list
</pre></div>
</div>
<p>Because the cluster is still in quorum all control plane operations can continue without issue. Let’s create a volume and mount it to confirm this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx-pvc</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">accessModes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteMany</span>
  <span class="nt">volumeMode</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Filesystem</span>
  <span class="nt">resources</span><span class="p">:</span>
  <span class="nt">requests</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">20Gi</span>
  <span class="nt">storageClassName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">px-shared-sc</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">containers</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">imagePullPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">ports</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">containerPort</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
    <span class="nt">protocol</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TCP</span>
  <span class="nt">volumeMounts</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">mountPath</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/usr/share/nginx/html</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">shared-data</span>
  <span class="nt">volumes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">shared-data</span>
  <span class="nt">persistentVolumeClaim</span><span class="p">:</span>
    <span class="nt">claimName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx-pvc</span>
  <span class="nt">restartPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Because our <cite>px-shared-sc</cite> has 2 replicas and we have 2 healthy nodes we can create the above volume and pod. Verify that it is running and ready:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po nginx
</pre></div>
</div>
<p>Now let’s try to create a volume with 3 replicas to see what happens:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">test-repl3-pvc</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">accessModes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="nt">resources</span><span class="p">:</span>
  <span class="nt">requests</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2Gi</span>
  <span class="nt">storageClassName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">px-postgres-sc</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>If you describe the PVC you will see an error about not being able to provision a volume with 3 replicas:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl describe pvc test-repl3-pvc
<span class="c1"># at the bottom of the output you will see something like the following:</span>
Events:
  Type     Reason              Age               From                         Message
  ----     ------              ----              ----                         -------
  Warning  ProvisioningFailed  6s <span class="o">(</span>x2 over 12s<span class="o">)</span>  persistentvolume-controller  Failed to provision volume with StorageClass <span class="s2">&quot;px-postgres-sc&quot;</span>: services <span class="s2">&quot;portworx-service&quot;</span> not found
  Warning  ProvisioningFailed  1s                persistentvolume-controller  Failed to provision volume with StorageClass <span class="s2">&quot;px-postgres-sc&quot;</span>: rpc error: <span class="nv">code</span> <span class="o">=</span> Internal <span class="nv">desc</span> <span class="o">=</span> Failed to create volume: could not find enough nodes to provision volume: <span class="m">1</span> out of <span class="m">1</span> pools could not be selected because they did not satisfy the following requirement: pool and its node must be online <span class="o">(</span>and not offline or full<span class="o">)</span>.
</pre></div>
</div>
<p>Meanwhile, the existing Postgres volume and Pod in the postgres namespace should still be operational even though a node is down. Let’s run a new benchmark in there to verify:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">POSTGRES_POD</span><span class="o">=</span><span class="sb">`</span>kubectl get po -n postgres -l <span class="nv">app</span><span class="o">=</span>postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].metadata.name}&#39;</span><span class="sb">`</span>
kubectl <span class="nb">exec</span> -it -n postgres <span class="nv">$POSTGRES_POD</span> -- psql -c <span class="s2">&quot;create database pxdemo5;&quot;</span>
kubectl <span class="nb">exec</span> -it -n postgres <span class="nv">$POSTGRES_POD</span> -- pgbench -i -s <span class="m">50</span> pxdemo5
</pre></div>
</div>
<p>So although we can’t create repl3 volumes we still can write to our existing volumes because they have consistent copies of the data that we can write to. Now, let’s inspect the postgres volume to see what is happening:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">VOLID</span><span class="o">=</span><span class="sb">`</span>kubectl pxc -n postgres pvc list <span class="p">|</span> grep postgres-data <span class="p">|</span> awk <span class="s1">&#39;{print $2}&#39;</span><span class="sb">`</span>
pxctl volume inspect <span class="nv">$VOLID</span>
</pre></div>
</div>
<p>In the output you will see that the volume’s <cite>Replication Status</cite> is showing as <cite>degraded</cite> an done of the nodes in the replica set is marked with an asterix, it should look something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt; Running pxctl on aws-node-3
  Volume             :  <span class="m">86399622933763755</span>
  Name               :  pvc-7f609f60-886a-4a6d-9ba3-2a741fe886a3
  Size               :  <span class="m">5</span>.0 GiB
  Format             :  ext4
  HA                 :  <span class="m">3</span>
  IO Priority        :  MEDIUM
  Creation <span class="nb">time</span>      :  Apr <span class="m">1</span> <span class="m">20</span>:39:47 UTC <span class="m">2021</span>
  Shared             :  no
  Status             :  up
  State              :  Attached: 9041032b-17a5-444a-aaec-6850983ccc49 <span class="o">(</span><span class="m">192</span>.168.1.11<span class="o">)</span>
  Last Attached      :  Apr <span class="m">6</span> <span class="m">22</span>:23:55 UTC <span class="m">2021</span>
  Device Path        :  /dev/pxd/pxd86399622933763755
  Labels             :  <span class="nv">repl</span><span class="o">=</span><span class="m">3</span>,app<span class="o">=</span>postgres,io_profile<span class="o">=</span>db_remote,namespace<span class="o">=</span>postgres,priority_io<span class="o">=</span>high,pvc<span class="o">=</span>postgres-data
  Mount Options             :  discard
  ProxyWrite          :  <span class="nb">false</span>
  Reads              :  <span class="m">1307</span>
  Reads MS           :  <span class="m">11212</span>
  Bytes Read         :  <span class="m">26361856</span>
  Writes             :  <span class="m">15580</span>
  Writes MS          :  <span class="m">1260568</span>
  Bytes Written      :  <span class="m">1558253568</span>
  IOs <span class="k">in</span> progress    :  <span class="m">0</span>
  Bytes used         :  <span class="m">2</span>.3 GiB
  Replica sets on nodes:
    Set <span class="m">0</span>
      Node      : <span class="m">192</span>.168.1.11 <span class="o">(</span>Pool 6ced6b19-4d86-4480-acb2-2357171ad188 <span class="o">)</span>
      Node      : <span class="m">192</span>.168.1.12 <span class="o">(</span>Pool 7f4c6218-32bc-48f3-bd86-92c3529a2447 <span class="o">)</span>*
      Node      : <span class="m">192</span>.168.1.13 <span class="o">(</span>Pool 5e1687b7-6a03-48d6-be6e-ca7a702e2d5f <span class="o">)</span>
  Replication Status   :  Degraded
  Volume consumers   :
    - Name           : postgres-ddf7d7dfc-thnx4 <span class="o">(</span>13deee41-7e72-4f56-a998-62a7d5c3ebef<span class="o">)</span> <span class="o">(</span>Pod<span class="o">)</span>
      Namespace      : postgres
      Running on     : aws-node-1
      Controlled by  : postgres-ddf7d7dfc <span class="o">(</span>ReplicaSet<span class="o">)</span>
</pre></div>
</div>
<p>Now let’s boot up our node and see that the volume will get resynced. Set up a watch on the volume inspec command to see the <cite>Replication Status</cite> transition from <cite>Degraded</cite> to <cite>Resync</cite> to <cite>Up</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl pxc pxctl volume inspect </span><span class="nv">$VOLID</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>In a separate window you can also watch the portworx node come back online:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl pxc pxctl status&quot;</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="ch_functional_testing.html" class="btn btn-neutral float-left" title="Functional Testing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Pure Storage Inc..
      <span class="lastupdated">
        Last updated on 2021-06-01 01:04.
      </span>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>