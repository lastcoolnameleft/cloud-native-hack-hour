

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Resilience &mdash; Portworx MS-GBB Lab Instructions 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/bespoke.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinxmark.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Monitoring" href="ch_monitoring.html" />
    <link rel="prev" title="Functional Testing" href="ch_functional_testing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Portworx MS-GBB Lab Instructions
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ch_prerequisites.html">Portworx Microsoft Global Blackbelts Lab Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch_functional_testing.html">Functional Testing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Resilience</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#node-failure">Node failure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#node-shutdown-no-quorum-lost">Node shutdown - no quorum lost</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-node-failure-quorum-lost">Multi-node Failure - quorum lost</a></li>
<li class="toctree-l2"><a class="reference internal" href="#disk-failure-simulation">Disk failure (simulation)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ch_monitoring.html">Monitoring</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Portworx MS-GBB Lab Instructions</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Resilience</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="resilience">
<h1>Resilience<a class="headerlink" href="#resilience" title="Permalink to this headline">¶</a></h1>
<p>In this section we will show how Portworx provides high availability in the case of node, network, and drive failures.</p>
<div class="section" id="node-failure">
<h2>Node failure<a class="headerlink" href="#node-failure" title="Permalink to this headline">¶</a></h2>
<p>When a Kubernetes node fails it can take up to 5 minutes for a pod to be rescheduled. This is due to a combination of the following default Kubernetes settings:</p>
<blockquote>
<div><ul class="simple">
<li><p>kubelet <cite>node-status-update-frequency</cite>: Specifies how often kubelet posts node status to master (default 10s)</p></li>
<li><p>kube-controller-manager <cite>node-monitor-period</cite>: The period for syncing NodeStatus in NodeController (default 5s)</p></li>
<li><p>kube-controller-manager <cite>node-monitor-grace-period</cite>: Amount of time which we allow running Node to be unresponsive before marking it unhealthy (default 40s)</p></li>
<li><p>kube-controller-manager <cite>pod-eviction-timeout</cite>: The grace period for deleting pods on failed nodes (default 5m0s)</p></li>
</ul>
</div></blockquote>
<p>5 minutes is too long to wait for a database to come back online, that is why Portworx provides a mechanism that allows for Pods to come back online much quicker. This is done by Stork when it is configured to schedule your pods. Remember the <cite>spec.stork.args.webhook-controller</cite> setting on your <cite>StorageCluster</cite> spec? That’s to ensure Stork is the scheduler on any Pod that has mounted a Portworx volume. Let’s check that it’s set to true:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get stc -n portworx cluster-1 -ojsonpath<span class="o">=</span><span class="s1">&#39;{.spec.stork.args.webhook-controller}&#39;</span>
</pre></div>
</div>
<p>If you the above command doesn’t return true that means you need to add the webhook controller arg:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">args</span><span class="o">=</span><span class="sb">`</span>kubectl get stc cluster-1 -n portworx -ojsonpath<span class="o">=</span><span class="s1">&#39;{.spec.stork.args}&#39;</span><span class="sb">`</span>
<span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;</span><span class="nv">$args</span><span class="s2">&quot;</span> <span class="o">]</span>
<span class="k">then</span>
  kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args&quot;, &quot;value&quot;: {&quot;webhook-controller&quot;: &quot;true&quot;}}]&#39;</span>
<span class="k">else</span>
  kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args/webhook-controller&quot;, &quot;value&quot;: &quot;true&quot;}]&#39;</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>Now that stork is configured as the scheduler for all pods using Portworx volumes, we can configure it with an aggressive <cite>health-monitor-interval</cite> setting of 30 seconds:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl patch stc cluster-1 -n portworx --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/stork/args/health-monitor-interval&quot;, &quot;value&quot;: &quot;30&quot;}]&#39;</span>
</pre></div>
</div>
<p>OK, so now we are all set to perform a node failover test.</p>
<p>First, let’s find out which node our postgres database is running on:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>kubectl get pods -l <span class="nv">app</span><span class="o">=</span>postgres -n postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].spec.nodeName}&#39;</span><span class="sb">`</span>
<span class="nb">echo</span> <span class="nv">$NODE</span>
</pre></div>
</div>
<p>To compare how fast we can failover a Pod with a portworx volume to a Pod that doesn’t have a Portworx volume we will deploy an nginx pod on the same host. What we want is to make sure nginx will initially run on the same node as postgres but then be free to move to any other node when the node fails. We will use a nodeSelector for this purpose. First, let’s label our node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl label node <span class="nv">$NODE</span> px-poc/run<span class="o">=</span><span class="s2">&quot;here&quot;</span>
</pre></div>
</div>
<p>Now let’s create an nginx deployment with the nodeSelector for that label:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">namespace</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">postgres</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">selector</span><span class="p">:</span>
  <span class="nt">matchLabels</span><span class="p">:</span>
    <span class="nt">app</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">template</span><span class="p">:</span>
  <span class="nt">metadata</span><span class="p">:</span>
    <span class="nt">labels</span><span class="p">:</span>
    <span class="nt">app</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">spec</span><span class="p">:</span>
    <span class="nt">nodeSelector</span><span class="p">:</span>
    <span class="nt">px-poc/run</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">here</span>
    <span class="nt">containers</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
    <span class="nt">imagePullPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
    <span class="nt">ports</span><span class="p">:</span>
    <span class="p p-Indicator">-</span> <span class="nt">containerPort</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
      <span class="nt">protocol</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TCP</span>
    <span class="nt">restartPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Now confirm that both pods are running on the same node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po -n postgres -owide
</pre></div>
</div>
<p>Now let’s add the <cite>px-poc/run=”here”</cite> label to at least one more worker node so that nginx can failover:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">OTHER_NODE</span><span class="o">=</span><span class="sb">`</span>kubectl get nodes -l node-role.kubernetes.io/worker<span class="o">=</span><span class="nb">true</span> --no-headers <span class="p">|</span> grep -v <span class="nv">$NODE</span> <span class="p">|</span> head -n <span class="m">1</span> <span class="p">|</span> awk <span class="s1">&#39;{print $1}&#39;</span><span class="sb">`</span>
kubectl label node <span class="nv">$OTHER_NODE</span> px-poc/run<span class="o">=</span><span class="s2">&quot;here&quot;</span>
</pre></div>
</div>
<p>Now we are ready to perform the resilience test, shutdown the node where nginx and postgres are running and see how fast they failover by running a watch command on:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch kubectl get po -owide -n postgres
</pre></div>
</div>
<p>In our test results below you can see by the age of the postgres pod that it failed over a 4m8s faster than nginx, the failover times in our test were 1m30s and 5m38s respectively:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>NAME                       READY   STATUS        RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
nginx-6598b75bbf-zqrxv     <span class="m">1</span>/1     Running       <span class="m">0</span>          18s     <span class="m">10</span>.44.0.5   aws-node-1   &lt;none&gt;           &lt;none&gt;
postgres-ddf7d7dfc-thnx4   <span class="m">1</span>/1     Running       <span class="m">0</span>          4m26s   <span class="m">10</span>.44.0.3   aws-node-1   &lt;none&gt;           &lt;none&gt;
</pre></div>
</div>
<p>Finally, let’s delete the nginx pod and remove our labels:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl delete deploy nginx -n postgres
kubectl label node <span class="nv">$NODE</span> <span class="nv">$OTHER_NODE</span> px-poc/run-
</pre></div>
</div>
</div>
<div class="section" id="node-shutdown-no-quorum-lost">
<h2>Node shutdown - no quorum lost<a class="headerlink" href="#node-shutdown-no-quorum-lost" title="Permalink to this headline">¶</a></h2>
<p>In the previous test we shutdown one of our hosts, because we installed on a minimum of 3 nodes we can verify that the cluster has not lost quorum:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl pxc node list
</pre></div>
</div>
<p>Because the cluster is still in quorum all control plane operations can continue without issue. Let’s create a volume and mount it to confirm this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx-pvc</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">accessModes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteMany</span>
  <span class="nt">volumeMode</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Filesystem</span>
  <span class="nt">resources</span><span class="p">:</span>
  <span class="nt">requests</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">20Gi</span>
  <span class="nt">storageClassName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">px-shared-sc</span>
<span class="nn">---</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">containers</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">imagePullPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx</span>
  <span class="nt">ports</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">containerPort</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
    <span class="nt">protocol</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TCP</span>
  <span class="nt">volumeMounts</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">mountPath</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">/usr/share/nginx/html</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">shared-data</span>
  <span class="nt">volumes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">shared-data</span>
  <span class="nt">persistentVolumeClaim</span><span class="p">:</span>
    <span class="nt">claimName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">nginx-pvc</span>
  <span class="nt">restartPolicy</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Always</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Because our <cite>px-shared-sc</cite> has 2 replicas and we have 2 healthy nodes we can create the above volume and pod. Verify that it is running and ready:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po nginx
</pre></div>
</div>
<p>Now let’s try to create a volume with 3 replicas to see what happens:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF | kubectl apply -f -</span>
<span class="l l-Scalar l-Scalar-Plain">kind</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">test-repl3-pvc</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">accessModes</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
  <span class="nt">resources</span><span class="p">:</span>
  <span class="nt">requests</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2Gi</span>
  <span class="nt">storageClassName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">px-postgres-sc</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>If you describe the PVC you will see an error about not being able to provision a volume with 3 replicas:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl describe pvc test-repl3-pvc
<span class="c1"># at the bottom of the output you will see something like the following:</span>
Events:
  Type     Reason              Age               From                         Message
  ----     ------              ----              ----                         -------
  Warning  ProvisioningFailed  6s <span class="o">(</span>x2 over 12s<span class="o">)</span>  persistentvolume-controller  Failed to provision volume with StorageClass <span class="s2">&quot;px-postgres-sc&quot;</span>: services <span class="s2">&quot;portworx-service&quot;</span> not found
  Warning  ProvisioningFailed  1s                persistentvolume-controller  Failed to provision volume with StorageClass <span class="s2">&quot;px-postgres-sc&quot;</span>: rpc error: <span class="nv">code</span> <span class="o">=</span> Internal <span class="nv">desc</span> <span class="o">=</span> Failed to create volume: could not find enough nodes to provision volume: <span class="m">1</span> out of <span class="m">1</span> pools could not be selected because they did not satisfy the following requirement: pool and its node must be online <span class="o">(</span>and not offline or full<span class="o">)</span>.
</pre></div>
</div>
<p>Meanwhile, the existing Postgres volume and Pod in the postgres namespace should still be operational even though a node is down. Let’s run a new benchmark in there to verify:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">POSTGRES_POD</span><span class="o">=</span><span class="sb">`</span>kubectl get po -n postgres -l <span class="nv">app</span><span class="o">=</span>postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].metadata.name}&#39;</span><span class="sb">`</span>
kubectl <span class="nb">exec</span> -it -n postgres <span class="nv">$POSTGRES_POD</span> -- psql -c <span class="s2">&quot;create database pxdemo5;&quot;</span>
kubectl <span class="nb">exec</span> -it -n postgres <span class="nv">$POSTGRES_POD</span> -- pgbench -i -s <span class="m">50</span> pxdemo5
</pre></div>
</div>
<p>So although we can’t create repl3 volumes we still can write to our existing volumes because they have consistent copies of the data that we can write to. Now, let’s inspect the postgres volume to see what is happening:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">VOLID</span><span class="o">=</span><span class="sb">`</span>kubectl pxc -n postgres pvc list <span class="p">|</span> grep postgres-data <span class="p">|</span> awk <span class="s1">&#39;{print $2}&#39;</span><span class="sb">`</span>
pxctl volume inspect <span class="nv">$VOLID</span>
</pre></div>
</div>
<p>In the output you will see that the volume’s <cite>Replication Status</cite> is showing as <cite>degraded</cite> an done of the nodes in the replica set is marked with an asterix, it should look something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt; Running pxctl on aws-node-3
  Volume             :  <span class="m">86399622933763755</span>
  Name               :  pvc-7f609f60-886a-4a6d-9ba3-2a741fe886a3
  Size               :  <span class="m">5</span>.0 GiB
  Format             :  ext4
  HA                 :  <span class="m">3</span>
  IO Priority        :  MEDIUM
  Creation <span class="nb">time</span>      :  Apr <span class="m">1</span> <span class="m">20</span>:39:47 UTC <span class="m">2021</span>
  Shared             :  no
  Status             :  up
  State              :  Attached: 9041032b-17a5-444a-aaec-6850983ccc49 <span class="o">(</span><span class="m">192</span>.168.1.11<span class="o">)</span>
  Last Attached      :  Apr <span class="m">6</span> <span class="m">22</span>:23:55 UTC <span class="m">2021</span>
  Device Path        :  /dev/pxd/pxd86399622933763755
  Labels             :  <span class="nv">repl</span><span class="o">=</span><span class="m">3</span>,app<span class="o">=</span>postgres,io_profile<span class="o">=</span>db_remote,namespace<span class="o">=</span>postgres,priority_io<span class="o">=</span>high,pvc<span class="o">=</span>postgres-data
  Mount Options             :  discard
  ProxyWrite          :  <span class="nb">false</span>
  Reads              :  <span class="m">1307</span>
  Reads MS           :  <span class="m">11212</span>
  Bytes Read         :  <span class="m">26361856</span>
  Writes             :  <span class="m">15580</span>
  Writes MS          :  <span class="m">1260568</span>
  Bytes Written      :  <span class="m">1558253568</span>
  IOs <span class="k">in</span> progress    :  <span class="m">0</span>
  Bytes used         :  <span class="m">2</span>.3 GiB
  Replica sets on nodes:
    Set <span class="m">0</span>
      Node      : <span class="m">192</span>.168.1.11 <span class="o">(</span>Pool 6ced6b19-4d86-4480-acb2-2357171ad188 <span class="o">)</span>
      Node      : <span class="m">192</span>.168.1.12 <span class="o">(</span>Pool 7f4c6218-32bc-48f3-bd86-92c3529a2447 <span class="o">)</span>*
      Node      : <span class="m">192</span>.168.1.13 <span class="o">(</span>Pool 5e1687b7-6a03-48d6-be6e-ca7a702e2d5f <span class="o">)</span>
  Replication Status   :  Degraded
  Volume consumers   :
    - Name           : postgres-ddf7d7dfc-thnx4 <span class="o">(</span>13deee41-7e72-4f56-a998-62a7d5c3ebef<span class="o">)</span> <span class="o">(</span>Pod<span class="o">)</span>
      Namespace      : postgres
      Running on     : aws-node-1
      Controlled by  : postgres-ddf7d7dfc <span class="o">(</span>ReplicaSet<span class="o">)</span>
</pre></div>
</div>
<p>Now let’s boot up our node and see that the volume will get resynced. Set up a watch on the volume inspec command to see the <cite>Replication Status</cite> transition from <cite>Degraded</cite> to <cite>Resync</cite> to <cite>Up</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl pxc pxctl volume inspect </span><span class="nv">$VOLID</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>In a separate window you can also watch the portworx node come back online:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl pxc pxctl status&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="multi-node-failure-quorum-lost">
<h2>Multi-node Failure - quorum lost<a class="headerlink" href="#multi-node-failure-quorum-lost" title="Permalink to this headline">¶</a></h2>
<p>Now we’re going to test how Portworx can recover quorum after it’s been lost. When you lose quorum the cluster as a whole cannot safely operate so you will not be able to create or use any volumes until quorum is re-established. What we are going to test here is how quickly that quorum can be re-established.</p>
<p>To lose quorum you have to lose n/2 + 1 of your storage nodes. Only storage nodes contribute to quorum in Portworx. So if you have a 3 worker-node cluster all you need to do is shutdown 2 of your nodes and see that on the remaining node Portworx will not be in quorum and won’t allow any operations.</p>
<p>After a a minute or so if you go to the surviving node you will see running the following output when running <cite>pxctl status</cite>, note that you have to pass in the <cite>-n &lt;surviving-node-name&gt;</cite> option to the pxctl command to be able to run <cite>pxctl status</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">NODE</span><span class="o">=</span>&lt;surviving-node-name&gt;
pxctl -n <span class="nv">$NODE</span> status
&gt;&gt; Running pxctl on aws-node-2
PX is not running on this host
List of last known failures:
Type  ID            Resource        Severity  Count  LastSeen    FirstSeen    Description
NODE  PortworxMonitorSchedulerInitializationFailed  aws-node-2        ALARM    <span class="m">1</span>  Apr <span class="m">7</span> <span class="m">23</span>:03:41 UTC <span class="m">2021</span>  Apr <span class="m">7</span> <span class="m">23</span>:03:41 UTC <span class="m">2021</span>  Could not init scheduler <span class="s1">&#39;kubernetes&#39;</span>: Could not find my node <span class="k">in</span> Kubernetes cluster: Get https://10.96.0.1:443/api/v1/nodes: dial tcp <span class="m">10</span>.96.0.1:443: i/o timeout
NODE  NodeStateChange          92257c6b-26f9-406a-afc2-4a7a454fd623  ALARM    <span class="m">1</span>  Apr <span class="m">7</span> <span class="m">23</span>:03:23 UTC <span class="m">2021</span>  Apr <span class="m">7</span> <span class="m">23</span>:03:23 UTC <span class="m">2021</span>  Node is not <span class="k">in</span> quorum. Waiting to connect to peer nodes on port <span class="m">9002</span>.
NODE  NodeStartFailure        92257c6b-26f9-406a-afc2-4a7a454fd623  ALARM    <span class="m">1</span>  Apr <span class="m">7</span> <span class="m">23</span>:02:27 UTC <span class="m">2021</span>  Apr <span class="m">7</span> <span class="m">23</span>:02:27 UTC <span class="m">2021</span>  Failed to start Portworx: failed to parse network interfaces: Network interface Mgmt Interface is down
NODE  InternalKvdbSetupFailed                  ALARM    <span class="m">1</span>  Apr <span class="m">7</span> <span class="m">22</span>:32:31 UTC <span class="m">2021</span>  Apr <span class="m">7</span> <span class="m">22</span>:32:31 UTC <span class="m">2021</span>  Failed to bootstrap internal kvdb: Failed to initialize k8s bootstrap: Failed to create configmap px-bootstrap-aws: Post https://10.96.0.1:443/api/v1/namespaces/kube-system/configmaps: dial tcp <span class="m">10</span>.96.0.1:443: i/o timeout
<span class="nb">command</span> terminated with <span class="nb">exit</span> code <span class="m">1</span>
</pre></div>
</div>
<p>Now let’s bring the nodes back online and watch as they get back in quorum. You can create a watch to see how quickly it happens:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl -n </span><span class="nv">$NODE</span><span class="s2"> pxc pxctl status&quot;</span>
</pre></div>
</div>
<p>The cluster should become fully operational after one or two minutes, depending on how fast your nodes boot up.</p>
</div>
<div class="section" id="disk-failure-simulation">
<h2>Disk failure (simulation)<a class="headerlink" href="#disk-failure-simulation" title="Permalink to this headline">¶</a></h2>
<p>To test disk failure we will put a pool into maintenance mode. What we will show is that in the case of the drive failing our postgres database will continue to operate without issue and the node where drive failure occurs will also continue to serve IO during the drive maintenance operation.</p>
<p>First, let’s find out where the Postgres pod is currently running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">NODE</span><span class="o">=</span><span class="sb">`</span>kubectl get pods -l <span class="nv">app</span><span class="o">=</span>postgres -n postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].spec.nodeName}&#39;</span><span class="sb">`</span>
<span class="nb">echo</span> <span class="nv">$NODE</span>
</pre></div>
</div>
<p>Now we can put the storage pool on that node into maintenance mode:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pxctl -n <span class="nv">$NODE</span> service pool maintenance --enter
</pre></div>
</div>
<p>Now the storage will appear as offline be the node will still be online, it will just be marked as <cite>StorageDown</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pxctl -n <span class="nv">$NODE</span> status
&gt;&gt; Running pxctl on aws-node-2
Status: PX storage down
License: Trial <span class="o">(</span>expires <span class="k">in</span> <span class="m">10</span> days<span class="o">)</span>
Node ID: 92257c6b-26f9-406a-afc2-4a7a454fd623
  IP: <span class="m">192</span>.168.1.12
   Local Storage Pool: <span class="m">1</span> pool
  POOL  IO_PRIORITY  RAID_LEVEL  USABLE  USED  STATUS    ZONE  REGION
  <span class="m">0</span>  MEDIUM    raid0    <span class="m">50</span> GiB  <span class="m">9</span>.0 GiB  In Maintenance  default  default
  Local Storage Devices: <span class="m">1</span> device
  Device  Path    Media Type    Size    Last-Scan
  <span class="m">0</span>:1  /dev/sdc  STORAGE_MEDIUM_MAGNETIC  <span class="m">50</span> GiB    <span class="m">07</span> Apr <span class="m">21</span> <span class="m">23</span>:20 UTC
  total      -      <span class="m">50</span> GiB
  Cache Devices:
   * No cache devices
  Kvdb Device:
  Device Path  Size
  /dev/sdd  <span class="m">64</span> GiB
   * Internal kvdb on this node is using this dedicated kvdb device to store its data.
Cluster Summary
  Cluster ID: aws
  Cluster UUID: edd05bd6-78fb-442a-adce-12f056c8b240
  Scheduler: kubernetes
  Nodes: <span class="m">3</span> node<span class="o">(</span>s<span class="o">)</span> with storage <span class="o">(</span><span class="m">2</span> online<span class="o">)</span>
  IP    ID          SchedulerNodeName  StorageNode  Used    Capacity  Status      StorageStatus      Version    Kernel      OS
  <span class="m">192</span>.168.1.12  92257c6b-26f9-406a-afc2-4a7a454fd623  aws-node-2    Yes    Unavailable  Unavailable  Online <span class="o">(</span>StorageDown<span class="o">)</span>  In Maintenance <span class="o">(</span>This node<span class="o">)</span>  <span class="m">2</span>.6.4.0-4f9e45f  <span class="m">4</span>.4.0-206-generic  Ubuntu <span class="m">16</span>.04.5 LTS
  <span class="m">192</span>.168.1.11  9041032b-17a5-444a-aaec-6850983ccc49  aws-node-1    Yes    <span class="m">10</span> GiB    <span class="m">50</span> GiB    Online      Up        <span class="m">2</span>.6.4.0-4f9e45f  <span class="m">4</span>.4.0-206-generic  Ubuntu <span class="m">16</span>.04.5 LTS
  <span class="m">192</span>.168.1.13  <span class="m">47585465</span>-2eb0-46d6-a2f5-45b3742a9783  aws-node-3    Yes    <span class="m">8</span>.6 GiB    <span class="m">50</span> GiB    Online      Up        <span class="m">2</span>.6.4.0-4f9e45f  <span class="m">4</span>.4.0-206-generic  Ubuntu <span class="m">16</span>.04.5 LTS
Global Storage Pool
  Total Used      :  <span class="m">19</span> GiB
  Total Capacity  :  <span class="m">100</span> GiB
</pre></div>
</div>
<p>Now let’s confirm that the postgres database didn’t need to be reschedule:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po -n postgres -l <span class="nv">app</span><span class="o">=</span>postgres -owide
</pre></div>
</div>
<p>It should still be running on the same node as before.  We can even access data on the database:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">POSTGRES_POD</span><span class="o">=</span><span class="sb">`</span>kubectl get po -n postgres -l <span class="nv">app</span><span class="o">=</span>postgres -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[0].metadata.name}&#39;</span><span class="sb">`</span>
kubectl <span class="nb">exec</span> -it -n postgres <span class="nv">$POSTGRES_POD</span> -- psql pxdemo -c <span class="s2">&quot;select count(*) from pgbench_accounts&quot;</span>
</pre></div>
</div>
<p>We can now bring our pool back online:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pxctl -n <span class="nv">$NODE</span> service pool maintenance --exit
</pre></div>
</div>
<p>When coming out of a maintenance state, after the drive is replaced (see maintenance operation above in test 3.5), the node will re-initialize and be offline for a short period of time:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>watch --color <span class="s2">&quot;kubectl pxc pxctl -n </span><span class="nv">$NODE</span><span class="s2"> status&quot;</span>
</pre></div>
</div>
<p>This can be done without relocating pods running on the node. If the pod tries to perform IO to the a volume on that node it will get an IO timeout error from the Linux file system. To be safe, we recommend relocating the pods during this maintenance operation as described in test 3.5. In this case we did not perform those steps and we can see the pod is still on the same node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl get po -n postgres -l <span class="nv">app</span><span class="o">=</span>postgres -owide
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="ch_monitoring.html" class="btn btn-neutral float-right" title="Monitoring" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="ch_functional_testing.html" class="btn btn-neutral float-left" title="Functional Testing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Pure Storage Inc..
      <span class="lastupdated">
        Last updated on 2021-06-02 02:41.
      </span>

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>